from config import URL, SCROLL_PAUSE_TIME
from utils import parse_post_time
import time
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import StaleElementReferenceException
from bs4 import BeautifulSoup
import webbrowser


class HoYoLabScraper:
    def __init__(self, db_manager):
        self.db_manager = db_manager
        self.driver = None  # å»¶é²åˆå§‹åŒ–

    def _init_driver(self):
        if self.driver is None:
            options = Options()
            options.add_experimental_option("detach", True)
            options.add_argument("--disable-blink-features=AutomationControlled")
            self.driver = webdriver.Chrome(options=options)
            self.driver.get(URL)

    # åŠ åœ¨é¡åˆ¥è£¡é¢
    def open_website(self):
        try:
            webbrowser.open(URL)
        except Exception as e:
            print(f"[éŒ¯èª¤] ç„¡æ³•é–‹å•Ÿç€è¦½å™¨: {e}")

    def get_article_timestamp(self, url):
        if self.driver is None:
            self._init_driver()

        try:
            self.driver.get(url)
            time.sleep(3)
            soup = BeautifulSoup(self.driver.page_source, 'html.parser')
            author_info = soup.find('p', class_='mhy-article-page-author-header__info')

            if author_info:
                raw_text = author_info.get_text(separator=' ', strip=True)
                post_time = parse_post_time(raw_text)
                if post_time:
                    print(f"ğŸ” è§£ææ™‚é–“: {post_time}")
                    self.driver.get(URL)
                    time.sleep(3)
                    return post_time
                else:
                    print("[è­¦å‘Š] ç„¡æ³•è§£æç™¼ä½ˆæ™‚é–“ï¼Œä½¿ç”¨ç›®å‰æ™‚é–“ä»£æ›¿")
            else:
                print("[è­¦å‘Š] æ‰¾ä¸åˆ° author_info ç¯€é»ï¼Œä½¿ç”¨ç›®å‰æ™‚é–“ä»£æ›¿")

            self.driver.get(URL)
            time.sleep(3)
        except Exception as e:
            print(f"[éŒ¯èª¤] æŠ“å–æ–‡ç« æ™‚é–“å¤±æ•—ï¼š{e}")
            try:
                self.driver.get(URL)
                time.sleep(3)
            except:
                pass
        return datetime.now()
    
    def start_scraping(self, existing_urls, save_callback, log_callback, stop_event):
        self._init_driver()  # é€™è£¡ç¢ºä¿driverå·²ç¶“å•Ÿå‹•

        existing_urls = self.db_manager.fetch_existing_urls()
        i = 0
        max_empty_scrolls = 3
        empty_scrolls = 0

        while not stop_event.is_set():
            try:
                wait = WebDriverWait(self.driver, 10)
                news_cards = wait.until(
                    EC.presence_of_all_elements_located((By.CLASS_NAME, "mhy-news-card"))
                )

                if i >= len(news_cards):
                    log_callback("\nğŸ”„ æ¨¡æ“¬æ»¾å‹•ï¼ŒæŠ“å–æ›´å¤šæ–‡ç« ä¸­...\n")
                    self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                    time.sleep(SCROLL_PAUSE_TIME)

                    new_cards = self.driver.find_elements(By.CLASS_NAME, "mhy-news-card")
                    if len(new_cards) <= len(news_cards):
                        empty_scrolls += 1
                    else:
                        empty_scrolls = 0

                    if empty_scrolls >= max_empty_scrolls:
                        log_callback("âœ… æ²’æœ‰æ›´å¤šæ–°æ–‡ç« ï¼ŒçµæŸæŠ“å–ã€‚\n")
                        break

                    continue

                card = news_cards[i]
                try:
                    link_element = card.find_element(By.TAG_NAME, "a")
                    raw_title = link_element.text.strip()
                    url = link_element.get_attribute("href")
                    real_title = raw_title.splitlines()[0].strip() if "\n" in raw_title else raw_title

                    if url not in existing_urls:
                        article_datetime = self.get_article_timestamp(url)
                        if self.db_manager.save_article(real_title, url, article_datetime.strftime("%Y-%m-%d %H:%M:%S")):
                            existing_urls.add(url)
                            log_callback(f"æ–°å¢æ–‡ç« : {real_title}\n")
                    else:
                        log_callback(f"[ç•¥é] å·²å­˜åœ¨ç¶²å€ï¼š{url}\n")
                except StaleElementReferenceException:
                    log_callback(f"[è­¦å‘Š] ç¬¬ {i+1} ç¯‡å¡ç‰‡å¤±æ•ˆï¼Œè·³é\n")
                except Exception as e:
                    log_callback(f"[éŒ¯èª¤] ç¬¬ {i+1} ç¯‡è™•ç†å¤±æ•—: {e}\n")

                i += 1

            except Exception as e:
                log_callback(f"[è‡´å‘½éŒ¯èª¤] ç„¡æ³•è¼‰å…¥å¡ç‰‡æ¸…å–®: {e}\n")
                break

        log_callback("ğŸ”š æŠ“å–æµç¨‹çµæŸ\n")

    def close(self):
        try:
            if self.driver:
                self.driver.quit()
                self.driver = None
        except Exception:
            pass