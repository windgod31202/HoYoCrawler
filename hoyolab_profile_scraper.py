from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import time
from datetime import datetime, timedelta  # è«‹ç¢ºä¿é€™å€‹åœ¨æª”æ¡ˆæœ€ä¸Šæ–¹å·²ç¶“æœ‰åŒ¯å…¥
import sqlite3
from config import TARGET_USER_POSTS_URL, SCROLL_PAUSE_TIME

def init_driver():
    options = Options()
    options.add_argument("--disable-blink-features=AutomationControlled")
    options.add_experimental_option("detach", True)
    return webdriver.Chrome(options=options)

def init_db(db_path="posts.db"):
    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()
    cursor.execute("""
        CREATE TABLE IF NOT EXISTS articles (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            url TEXT UNIQUE,
            title TEXT,
            author TEXT,
            date TEXT,
            content TEXT
        )
    """)
    conn.commit()
    return conn

def add_article_to_db(conn, url, title, author, date, content):
    cursor = conn.cursor()
    try:
        cursor.execute('''
            INSERT INTO articles (url, title, author, date, content)
            VALUES (?, ?, ?, ?, ?)
        ''', (url, title, author, date, content))
        conn.commit()
        return True
    except sqlite3.IntegrityError:
        return False

def wait_for_article_cards(driver, timeout=10):
    try:
        WebDriverWait(driver, timeout).until(
            EC.presence_of_all_elements_located(
                (By.CSS_SELECTOR, "div.mhy-article-card")
            )
        )
        print("[æˆåŠŸ] æ‰¾åˆ°æ–‡ç« å¡ç‰‡")
    except:
        print("[éŒ¯èª¤] è¶…æ™‚æ‰¾ä¸åˆ°æ–‡ç« å¡ç‰‡")

def scroll_to_bottom(driver, pause_time=1.5):
    driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
    time.sleep(pause_time)

def extract_post_links(driver):
    try:
        WebDriverWait(driver, 10).until(
            EC.presence_of_all_elements_located(
                (By.CSS_SELECTOR, "div.mhy-article-card-wrapper.mhy-account-center-post-card")
            )
        )
    except:
        print("[éŒ¯èª¤] æœªæ‰¾åˆ°æ–‡ç« å¡ç‰‡å…ƒç´ ï¼Œå°‡å˜—è©¦æƒæå…¨éƒ¨ <a> é€£çµ")
    soup = BeautifulSoup(driver.page_source, "html.parser")
    links = set()
    outer_cards = soup.select("div.mhy-article-card-wrapper.mhy-account-center-post-card")
    for outer_card in outer_cards:
        inner_card = outer_card.select_one("div.mhy-article-card")
        if inner_card:
            a_tag = inner_card.find("a", href=True)
            if a_tag:
                href = a_tag['href']
                if href.startswith("/article/") and href[9:].isdigit():
                    full_url = "https://www.hoyolab.com" + href
                    links.add(full_url)
    a_tags = soup.find_all("a", href=True)
    for a in a_tags:
        href = a['href']
        if href.startswith("/article/") and href[9:].isdigit():
            full_url = "https://www.hoyolab.com" + href
            links.add(full_url)
    return list(links)

def extract_post_details(driver, url):
    WAIT_TIMEOUT = 15
    try:
        driver.get(url)
        wait = WebDriverWait(driver, WAIT_TIMEOUT)
        time.sleep(2)

        title_element = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "h1"))
        )
        title = title_element.text.strip()

        content_element = wait.until(
            EC.presence_of_element_located((By.CSS_SELECTOR, "div.mhy-article-page__content"))
        )

        # ä½¿ç”¨ BeautifulSoup è™•ç†å®Œæ•´ HTML å…§å®¹
        soup = BeautifulSoup(content_element.get_attribute("innerHTML"), "html.parser")


        # è™•ç†æ‰€æœ‰åœ–ç‰‡æ¨™ç±¤ï¼Œä¸é™ class çµæ§‹
        img_count = 0
        for img in soup.find_all("img"):
            src = img.get("data-src") or img.get("src")
            if src and src.startswith("http"):
                img.insert_after(soup.new_string(f"\n[åœ–ç‰‡]: {src}\n"))
                img_count += 1
        print(f"âœ… å…±ç™¼ç¾ {img_count} å¼µåœ–ç‰‡")

        # âœ… è™•ç† YouTube iframe
        youtube_count = 0
        for iframe in soup.select("iframe.mhy-video-frame.ql-frame"):
            src = iframe.get("src")
            if src and "youtube" in src:
                iframe.insert_after(soup.new_string(f"\n[YouTubeå½±ç‰‡]: {src}\n"))
                youtube_count += 1
        print(f"âœ… å…±ç™¼ç¾ {youtube_count} å‰‡ YouTube å½±ç‰‡")

        # âœ… å–å¾—ç´”æ–‡å­—ï¼ˆå«åœ–ç‰‡èˆ‡å½±ç‰‡çš„é™„è¨»ï¼‰
        content = soup.get_text("\n").strip()

        # ä½œè€…
        try:
            author_element = WebDriverWait(driver, WAIT_TIMEOUT).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, ".mhy-account-title__name"))
            )
            author = author_element.text.strip()
        except:
            author = "[æœªçŸ¥]"

        # æ™‚é–“è™•ç†
        try:
            date_element = WebDriverWait(driver, WAIT_TIMEOUT).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "p.mhy-article-page-author-header__info"))
            )
            date_raw = date_element.text.strip()
            now = datetime.now()

            if "å‰›å‰›" in date_raw or "åˆ†é˜å‰" in date_raw or "å°æ™‚å‰" in date_raw:
                date = now.strftime("%Y/%m/%d")
            elif "å¤©å‰" in date_raw:
                days_ago = int(date_raw.replace("å¤©å‰", "").strip())
                date = (now - timedelta(days=days_ago)).strftime("%Y/%m/%d")
            elif "/" in date_raw and date_raw.count("/") == 1:
                date = f"{now.year}/{date_raw}"
            else:
                date = date_raw
        except:
            date = "[æœªçŸ¥]"

        # é™¤éŒ¯åˆ—å°
        print(f"\nğŸ”— æ–‡ç« ç¶²å€: {url}")
        print("ğŸ“Œ æ¨™é¡Œ:", title)
        print("ğŸ‘¤ ä½œè€…:", author)
        print("ğŸ“… æ—¥æœŸ:", date)
        print("ğŸ“ å…§æ–‡:", content[:200] + "..." if content else "[å…§æ–‡ç¼ºå¤±]")
        print("-" * 80)

        return title, author, date, content

    except Exception as e:
        print(f"[éŒ¯èª¤] ç„¡æ³•æŠ“å–æ–‡ç«  {url}ï¼ŒéŒ¯èª¤åŸå› ï¼š{e}")
        print("-" * 80)
        return None, None, None, None
    
def scroll_and_collect_all_links(driver, max_scrolls=100, pause_time=2, max_no_growth=5):
    all_seen_links = set()
    no_growth_count = 0

    for i in range(max_scrolls):
        # æ»¾å‹•åº•éƒ¨ä»¥è§¸ç™¼è¼‰å…¥
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(pause_time)

        # å½ä¸‹æ‹‰è£œå……æ»¾å‹•è§¸ç™¼
        driver.execute_script("window.scrollBy(0, -200);")
        time.sleep(1)

        # æŠ“å–ç›®å‰é€£çµ
        new_links = set(extract_post_links(driver))

        if len(new_links) > len(all_seen_links):
            newly_added = new_links - all_seen_links
            all_seen_links.update(new_links)
            print(f"ç¬¬ {i+1} æ¬¡æ»¾å‹•ï¼Œç™¼ç¾ {len(newly_added)} å€‹æ–°é€£çµï¼ˆç´¯è¨ˆï¼š{len(all_seen_links)}ï¼‰")
            no_growth_count = 0
        else:
            no_growth_count += 1
            print(f"ç¬¬ {i+1} æ¬¡æ»¾å‹•ç„¡æ–°å¢ï¼Œé€£çºŒ {no_growth_count} æ¬¡ç„¡å¢é•·")

        # âœ… æª¢æŸ¥æ˜¯å¦å·²ç¶“åˆ°åº•éƒ¨
        try:
            footer_text = driver.find_element(By.CLASS_NAME, "mhy-load-next-core-content").text
            if "å·²ç¶“æ‹‰åˆ°åº•äº†" in footer_text:
                print("âœ… åµæ¸¬åˆ°åº•éƒ¨è¨Šæ¯ã€å·²ç¶“æ‹‰åˆ°åº•äº†ã€ï¼ŒçµæŸæ»¾å‹•")
                break
        except:
            pass  # å¯èƒ½é‚„æ²’å‡ºç¾ï¼Œå¿½ç•¥

        # âš ï¸ ç•¶ã€Œé»æ“Šè¼‰å…¥æ›´å¤šã€å­˜åœ¨ï¼Œè¡¨ç¤ºé‚„èƒ½è¼‰å…¥ â†’ ä¸ä¸­æ­¢
        try:
            more_text = driver.find_element(By.CLASS_NAME, "next-content").text
            if "é»æ“Šè¼‰å…¥æ›´å¤š" in more_text:
                print("ğŸ”„ åµæ¸¬åˆ°ã€é»æ“Šè¼‰å…¥æ›´å¤šã€å­˜åœ¨ï¼Œç¹¼çºŒæ»¾å‹•")
                continue
        except:
            pass  # æ²’å‡ºç¾å°±ç•¥é

        # ğŸš« è‹¥é€£çºŒå¤šæ¬¡ç„¡å¢é•·å°±æå‰çµ‚æ­¢
        if no_growth_count >= max_no_growth:
            print("âš ï¸ åµæ¸¬åˆ°é€£çºŒå¤šæ¬¡ç„¡æ–°å¢é€£çµï¼Œææ—©çµæŸæ»¾å‹•")
            break

    return list(all_seen_links)

def update_article_content(conn, url, new_content):
    cursor = conn.cursor()
    cursor.execute("UPDATE articles SET content = ? WHERE url = ?", (new_content, url))
    conn.commit()

def fill_missing_article_content(conn, driver):
    cursor = conn.cursor()
    cursor.execute("SELECT url FROM articles WHERE content IS NULL OR TRIM(content) = ''")
    rows = cursor.fetchall()

    if not rows:
        print("âœ… æ‰€æœ‰æ–‡ç« éƒ½æœ‰å…§å®¹ï¼Œä¸éœ€è£œæŠ“ã€‚")
        return

    print(f"ğŸ” ç™¼ç¾ {len(rows)} ç¯‡æ–‡ç« ç¼ºå°‘å…§æ–‡ï¼Œé–‹å§‹è£œæŠ“...")

    for (url,) in rows:
        title, author, date, content = extract_post_details(driver, url)
        if content:
            update_article_content(conn, url, content)
            print(f"âœ… å·²è£œä¸Šæ–‡ç« ï¼š{url}")
        else:
            print(f"âš ï¸ ç„¡æ³•è£œä¸Šï¼š{url}ï¼ˆå¯èƒ½å·²å¤±æ•ˆï¼‰")
        time.sleep(1)

def update_missing_images(conn, driver):
    cursor = conn.cursor()
    cursor.execute("SELECT url, content FROM articles")
    rows = cursor.fetchall()

    candidates = []
    for url, content in rows:
        if "[åœ–ç‰‡]:" not in (content or ""):
            candidates.append(url)

    if not candidates:
        print("âœ… æ‰€æœ‰æ–‡ç« éƒ½æœ‰åœ–ç‰‡æˆ–å·²ç¢ºèªç„¡åœ–ã€‚")
        return

    print(f"ğŸ” æº–å‚™æª¢æŸ¥ {len(candidates)} ç¯‡å¯èƒ½éºæ¼åœ–ç‰‡çš„æ–‡ç« ...")

    for url in candidates:
        print(f"ğŸ” é‡æ–°æª¢æŸ¥åœ–ç‰‡ï¼š{url}")
        title, author, date, new_content = extract_post_details(driver, url)
        if new_content and "[åœ–ç‰‡]:" in new_content:
            update_article_content(conn, url, new_content)
            print(f"âœ… å·²è£œä¸Šåœ–ç‰‡ï¼š{url}")
        else:
            print(f"âš ï¸ æ²’æ‰¾åˆ°åœ–ç‰‡ï¼š{url}")
        time.sleep(1)


def main():
    conn = init_db()
    driver = init_driver()

    try:
        cursor = conn.cursor()
        cursor.execute("SELECT url FROM articles")
        db_links = set(row[0] for row in cursor.fetchall())

        driver.get(TARGET_USER_POSTS_URL)
        time.sleep(3)

        all_links_seen = scroll_and_collect_all_links(driver, max_scrolls=10000, pause_time=SCROLL_PAUSE_TIME, max_no_growth=5)
        new_links = [link for link in all_links_seen if link not in db_links]
        print(f"ğŸ” å¾å…± {len(all_links_seen)} ç¯‡æ–‡ç« ä¸­ç™¼ç¾ {len(new_links)} ç¯‡æ–°æ–‡ç« ")

        for link in new_links:
            title, author, date, content = extract_post_details(driver, link)
            if title:
                added = add_article_to_db(conn, link, title, author, date, content)
                if added:
                    db_links.add(link)
            time.sleep(1)

        print(f"\nâœ… æŠ“å–å®Œæˆï¼Œç›®å‰è³‡æ–™åº«å…± {len(db_links)} ç¯‡æ–‡ç« ")

        # âœ… è£œæŠ“éºæ¼å…§å®¹
        fill_missing_article_content(conn, driver)
        # âœ… è£œæŠ“éºæ¼çš„åœ–ç‰‡ï¼ˆcontent è£¡é¢æ²’åŒ…å«åœ–ç‰‡é€£çµï¼‰
        update_missing_images(conn, driver)

    finally:
        driver.quit()
        conn.close()

if __name__ == "__main__":
    main()
